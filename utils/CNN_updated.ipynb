{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54a76bda",
      "metadata": {
        "id": "54a76bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5679716c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5679716c",
        "outputId": "a0fe2d65-c549-49aa-d2e7-52d37787a73f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 2.10.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers, mixed_precision\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import tensorflow_addons as tfa\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print('TensorFlow', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fe9aa1",
      "metadata": {
        "id": "f3fe9aa1"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '../data'   # change to the parent folder that contains 'train' and 'test' or 'train' only\n",
        "TRAIN_SUBDIR = 'train' # or 'images_train'\n",
        "TEST_SUBDIR = 'test'   # optional if you have a separate test folder\n",
        "IMG_SIZE = (100, 100)  # input size used previously\n",
        "BATCH_SIZE = 32\n",
        "SEED = 1337\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "NUM_EPOCHS = 30\n",
        "NUM_CLASSES = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a785da3a",
      "metadata": {
        "id": "a785da3a"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(DATA_DIR, TRAIN_SUBDIR)\n",
        "test_dir = os.path.join(DATA_DIR, TEST_SUBDIR) if os.path.isdir(os.path.join(DATA_DIR, TEST_SUBDIR)) else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "92ee1754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ee1754",
        "outputId": "60ffd47e-dffd-4c47-b312-538800fb6ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28800 files belonging to 36 classes.\n",
            "Found 7236 files belonging to 36 classes.\n"
          ]
        }
      ],
      "source": [
        "# If you don't have a separate test folder, we use validation_split\n",
        "if test_dir is None:\n",
        "    train_ds = image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMG_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        validation_split=0.2,\n",
        "        subset='training',\n",
        "        seed=SEED\n",
        "    )\n",
        "    val_ds = image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMG_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        validation_split=0.2,\n",
        "        subset='validation',\n",
        "        seed=SEED\n",
        "    )\n",
        "else:\n",
        "    train_ds = image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMG_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        seed=SEED\n",
        "    )\n",
        "    val_ds = image_dataset_from_directory(\n",
        "        test_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMG_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        seed=SEED\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "34336a65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34336a65",
        "outputId": "b63305c5-e732-4cb6-e667-04a50dd2afc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "class_names = train_ds.class_names\n",
        "NUM_CLASSES = len(class_names)\n",
        "print('Classes:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f77a0790",
      "metadata": {
        "id": "f77a0790"
      },
      "outputs": [],
      "source": [
        "# Build a normalization layer (scales to [0,1])\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.25),  # Increased rotation\n",
        "    layers.RandomZoom(0.2),        # Increased zoom\n",
        "    layers.RandomContrast(0.2),    # Increased contrast\n",
        "    layers.RandomBrightness(0.2),  # Increased brightness\n",
        "    layers.RandomTranslation(0.15, 0.15),  # More translation\n",
        "], name='data_augmentation')\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y),\n",
        "                        num_parallel_calls=AUTOTUNE)\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d4957fe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d4957fe5",
        "outputId": "e924dd94-7f44-4863-8b32-cb24c5cb61d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"improved_cnn_isl\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
            "                                                                 \n",
            " data_augmentation (Sequenti  (None, 100, 100, 1)      0         \n",
            " al)                                                             \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 100, 100, 1)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 100, 100, 64)      640       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 100, 100, 64)     256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 100, 100, 64)      36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 100, 100, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 50, 50, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 50, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 50, 50, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 50, 50, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 50, 50, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 50, 50, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 25, 25, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25, 25, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 25, 25, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 25, 25, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 25, 25, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 25, 25, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 12, 12, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 256)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 36)                9252      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,423,076\n",
            "Trainable params: 1,419,748\n",
            "Non-trainable params: 3,328\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_improved_model(input_shape=(100, 100, 1), num_classes=36):\n",
        "    reg = regularizers.l2(1e-4)\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = normalization_layer(x)\n",
        "\n",
        "    # Block 1 - More filters to capture features\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = layers.Conv2D(256, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(256, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs, outputs, name='improved_cnn_isl')\n",
        "    return model\n",
        "\n",
        "model = build_improved_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5a99e3f7",
      "metadata": {
        "id": "5a99e3f7"
      },
      "outputs": [],
      "source": [
        "# Compile model with a small weight decay via AdamW-style optimizer\n",
        "initial_learning_rate = 1e-3\n",
        "decay_steps = len(train_ds) * 50  # 50 epochs\n",
        "\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',  # Changed from val_loss\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "earlystop_cb = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,  # Increased patience\n",
        "    restore_best_weights=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,  # Less aggressive reduction\n",
        "    patience=7,\n",
        "    min_lr=1e-7,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "\n",
        "cbs = [checkpoint_cb, earlystop_cb, reduce_lr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Ads5_9OC1iAg",
      "metadata": {
        "id": "Ads5_9OC1iAg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "689e6e6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "689e6e6d",
        "outputId": "e70cce19-3453-46c0-ceaf-5f1e36322663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "900/900 [==============================] - 592s 581ms/step - loss: 4.1894 - accuracy: 0.0318 - val_loss: 3.9294 - val_accuracy: 0.0174 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "900/900 [==============================] - 514s 571ms/step - loss: 3.7144 - accuracy: 0.0489 - val_loss: 3.3138 - val_accuracy: 0.1177 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "900/900 [==============================] - 516s 574ms/step - loss: 3.3514 - accuracy: 0.0999 - val_loss: 3.7809 - val_accuracy: 0.0554 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "900/900 [==============================] - 517s 574ms/step - loss: 3.1388 - accuracy: 0.1455 - val_loss: 3.0425 - val_accuracy: 0.1685 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "900/900 [==============================] - 517s 574ms/step - loss: 2.9566 - accuracy: 0.1912 - val_loss: 3.0727 - val_accuracy: 0.1647 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "900/900 [==============================] - 516s 573ms/step - loss: 2.6632 - accuracy: 0.2694 - val_loss: 2.6236 - val_accuracy: 0.2757 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "900/900 [==============================] - 516s 574ms/step - loss: 2.3794 - accuracy: 0.3526 - val_loss: 1.7406 - val_accuracy: 0.5155 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "900/900 [==============================] - 516s 574ms/step - loss: 2.1568 - accuracy: 0.4205 - val_loss: 3.2973 - val_accuracy: 0.1715 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "900/900 [==============================] - 517s 574ms/step - loss: 1.9593 - accuracy: 0.4852 - val_loss: 1.2546 - val_accuracy: 0.6863 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "900/900 [==============================] - 516s 574ms/step - loss: 1.8161 - accuracy: 0.5230 - val_loss: 1.2729 - val_accuracy: 0.6546 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "900/900 [==============================] - 515s 572ms/step - loss: 1.7265 - accuracy: 0.5511 - val_loss: 11.0559 - val_accuracy: 0.0424 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "900/900 [==============================] - 516s 573ms/step - loss: 1.6332 - accuracy: 0.5818 - val_loss: 3.1869 - val_accuracy: 0.2096 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "900/900 [==============================] - 515s 572ms/step - loss: 1.5799 - accuracy: 0.5975 - val_loss: 0.9146 - val_accuracy: 0.7663 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "900/900 [==============================] - 511s 568ms/step - loss: 1.5251 - accuracy: 0.6147 - val_loss: 1.1271 - val_accuracy: 0.7019 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "900/900 [==============================] - 554s 616ms/step - loss: 1.4855 - accuracy: 0.6247 - val_loss: 0.7428 - val_accuracy: 0.8295 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "900/900 [==============================] - 511s 567ms/step - loss: 1.4409 - accuracy: 0.6381 - val_loss: 2.6770 - val_accuracy: 0.3372 - lr: 0.0010\n",
            "Epoch 17/30\n",
            "900/900 [==============================] - 511s 567ms/step - loss: 1.4136 - accuracy: 0.6449 - val_loss: 0.7959 - val_accuracy: 0.8111 - lr: 0.0010\n",
            "Epoch 18/30\n",
            "900/900 [==============================] - 511s 568ms/step - loss: 1.3858 - accuracy: 0.6522 - val_loss: 0.8208 - val_accuracy: 0.8122 - lr: 0.0010\n",
            "Epoch 19/30\n",
            "900/900 [==============================] - 514s 571ms/step - loss: 1.3604 - accuracy: 0.6608 - val_loss: 1.2260 - val_accuracy: 0.6751 - lr: 0.0010\n",
            "Epoch 20/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.3442 - accuracy: 0.6651 - val_loss: 0.9184 - val_accuracy: 0.7713 - lr: 0.0010\n",
            "Epoch 21/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.3318 - accuracy: 0.6699 - val_loss: 0.6603 - val_accuracy: 0.8462 - lr: 0.0010\n",
            "Epoch 22/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.2943 - accuracy: 0.6779 - val_loss: 5.3988 - val_accuracy: 0.1740 - lr: 0.0010\n",
            "Epoch 23/30\n",
            "900/900 [==============================] - 511s 568ms/step - loss: 1.2938 - accuracy: 0.6789 - val_loss: 2.3934 - val_accuracy: 0.3776 - lr: 0.0010\n",
            "Epoch 24/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.2817 - accuracy: 0.6831 - val_loss: 0.9823 - val_accuracy: 0.7483 - lr: 0.0010\n",
            "Epoch 25/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.2653 - accuracy: 0.6908 - val_loss: 1.2216 - val_accuracy: 0.6917 - lr: 0.0010\n",
            "Epoch 26/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 1.2672 - accuracy: 0.6867 - val_loss: 1.9432 - val_accuracy: 0.4925 - lr: 0.0010\n",
            "Epoch 27/30\n",
            "900/900 [==============================] - 511s 568ms/step - loss: 1.2504 - accuracy: 0.6950 - val_loss: 0.7963 - val_accuracy: 0.8064 - lr: 0.0010\n",
            "Epoch 28/30\n",
            "900/900 [==============================] - 511s 568ms/step - loss: 1.2538 - accuracy: 0.6928 - val_loss: 3.7198 - val_accuracy: 0.2159 - lr: 0.0010\n",
            "Epoch 29/30\n",
            "900/900 [==============================] - 512s 568ms/step - loss: 1.0429 - accuracy: 0.7502 - val_loss: 1.0639 - val_accuracy: 0.7055 - lr: 5.0000e-04\n",
            "Epoch 30/30\n",
            "900/900 [==============================] - 512s 569ms/step - loss: 0.9853 - accuracy: 0.7595 - val_loss: 0.4467 - val_accuracy: 0.9060 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=cbs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3835a545",
      "metadata": {
        "id": "3835a545"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy and loss\n",
        "import matplotlib.pyplot as plt\n",
        "hist = history.history\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist['loss'], label='train_loss')\n",
        "plt.plot(hist['val_loss'], label='val_loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist['accuracy'], label='train_acc')\n",
        "plt.plot(hist['val_accuracy'], label='val_acc')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c91d9d2",
      "metadata": {
        "id": "0c91d9d2"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation/test set\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(f'Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b00747",
      "metadata": {
        "id": "22b00747"
      },
      "outputs": [],
      "source": [
        "# Get predictions and build confusion matrix\n",
        "# Collect all images and labels from val_ds\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for x_batch, y_batch in val_ds:\n",
        "    preds = model.predict(x_batch)\n",
        "    y_true.extend(np.argmax(y_batch, axis=1).tolist())\n",
        "    y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print('Confusion matrix shape:', cm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db80f095",
      "metadata": {
        "id": "db80f095"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):\n",
        "    plt.figure(figsize=(10,8))\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-12)\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        val = cm[i, j]\n",
        "        if normalize:\n",
        "            fmt = '{:.2f}'.format(val)\n",
        "        else:\n",
        "            fmt = str(int(val))\n",
        "        plt.text(j, i, fmt, horizontalalignment='center',\n",
        "                    color='white' if val > thresh else 'black', fontsize=6)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "plot_confusion_matrix(cm, class_names, normalize=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e5cab5",
      "metadata": {
        "id": "f5e5cab5"
      },
      "outputs": [],
      "source": [
        "# Classification report (per-class precision/recall/f1)\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "# Visualize misclassifications\n",
        "mis_idx = [i for i, (a,b) in enumerate(zip(y_true, y_pred)) if a != b]\n",
        "sample_idx = random.choice(mis_idx)\n",
        "plt.imshow(list(val_ds.unbatch().map(lambda x,y: x))[sample_idx].numpy().squeeze(), cmap='gray')\n",
        "plt.title(f\"True: {class_names[y_true[sample_idx]]}, Pred: {class_names[y_pred[sample_idx]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tlFNI-Q1xrn",
      "metadata": {
        "id": "_tlFNI-Q1xrn"
      },
      "outputs": [],
      "source": [
        "def predict_with_tta(model, dataset, num_augmentations=5):\n",
        "    \"\"\"Apply test-time augmentation for more robust predictions\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(num_augmentations):\n",
        "        preds = model.predict(dataset)\n",
        "        predictions.append(preds)\n",
        "\n",
        "    # Average predictions\n",
        "    final_preds = np.mean(predictions, axis=0)\n",
        "    return np.argmax(final_preds, axis=1)\n",
        "\n",
        "# Use for evaluation\n",
        "y_pred_tta = predict_with_tta(model, val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DGlclzmb1zzW",
      "metadata": {
        "id": "DGlclzmb1zzW"
      },
      "outputs": [],
      "source": [
        "# Analyze confusion specifically for problematic classes\n",
        "problem_classes = ['c', 'f', 'm', 'o']\n",
        "problem_indices = [class_names.index(c) for c in problem_classes]\n",
        "\n",
        "for idx in problem_indices:\n",
        "    class_predictions = cm[idx]\n",
        "    confused_with = np.argsort(class_predictions)[-5:]  # Top 5 confusions\n",
        "    print(f\"\\nClass '{class_names[idx]}' confused with:\")\n",
        "    for conf_idx in confused_with:\n",
        "        if conf_idx != idx:\n",
        "            print(f\"  {class_names[conf_idx]}: {class_predictions[conf_idx]} times\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
